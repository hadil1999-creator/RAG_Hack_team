{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (1.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (2.31.0)\n",
      "Requirement already satisfied: tiktoken in c:\\python312\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in c:\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python312\\lib\\site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\python312\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\21629\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai requests tiktoken numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve key and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://finetuneai.openai.azure.com/\n",
      "52764c4f08f446f392342880499e7434\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "#load variable from .env file\n",
    "load_dotenv()\n",
    "# Set the environment variable\n",
    "load_dotenv()\n",
    "azure_oai_endpoint = os.getenv(\"AZURE_OAI_FINETUNE_ENDPOINT\")\n",
    "azure_oai_key = os.getenv(\"AZURE_OAI_FINETUNE_KEY\")\n",
    "azure_oai_deployment = os.getenv(\"AZURE_OAI_FINETUNE_DEPLOYMENT\")\n",
    "\n",
    "\n",
    "# Verify that it has been set\n",
    "print(azure_oai_endpoint)\n",
    "# Verify that it has been set\n",
    "print(azure_oai_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in training set: 45\n",
      "First example in training set:\n",
      "{'role': 'system', 'content': 'You are a helper in fianance and an advisor in investement'}\n",
      "{'role': 'user', 'content': 'How do I build credit?'}\n",
      "{'role': 'assistant', 'content': 'Borrow money and actually pay it back. Revolutionary, isnâ€™t it?'}\n",
      "\n",
      "Number of examples in validation set: 12\n",
      "First example in validation set:\n",
      "{'role': 'system', 'content': 'You are a helper in fianance and an advisor in investement'}\n",
      "{'role': 'user', 'content': 'How do I save for retirement?'}\n",
      "{'role': 'assistant', 'content': \"Just start putting money away now and hope you don't need it until you're 80.\"}\n"
     ]
    }
   ],
   "source": [
    "# Run preliminary checks\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the training set\n",
    "with open(r'C:\\Users\\21629\\Desktop\\new\\train_data.jsonl', 'r', encoding='utf-8') as f:\n",
    "    training_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Training dataset stats\n",
    "print(\"Number of examples in training set:\", len(training_dataset))\n",
    "print(\"First example in training set:\")\n",
    "for message in training_dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Load the validation set\n",
    "with open(r'C:\\Users\\21629\\Desktop\\new\\val_data.jsonl', 'r', encoding='utf-8') as f:\n",
    "    validation_dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Validation dataset stats\n",
    "print(\"\\nNumber of examples in validation set:\", len(validation_dataset))\n",
    "print(\"First example in validation set:\")\n",
    "for message in validation_dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\21629\\Desktop\\new\\train_data.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 46, 61\n",
      "mean / median: 52.044444444444444, 52.0\n",
      "p5 / p95: 46.0, 60.599999999999994\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 9, 26\n",
      "mean / median: 16.88888888888889, 17.0\n",
      "p5 / p95: 12.0, 24.799999999999997\n",
      "**************************************************\n",
      "Processing file: C:\\Users\\21629\\Desktop\\new\\val_data.jsonl\n",
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 46, 58\n",
      "mean / median: 52.333333333333336, 52.5\n",
      "p5 / p95: 47.1, 57.45\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 14, 25\n",
      "mean / median: 18.416666666666668, 18.0\n",
      "p5 / p95: 14.0, 24.45\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if not isinstance(message, dict):\n",
    "            print(f\"Unexpected message format: {message}\")\n",
    "            continue\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            if not isinstance(value, str):\n",
    "                print(f\"Unexpected value type for key '{key}': {value}\")\n",
    "                continue\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if not isinstance(message, dict):\n",
    "            print(f\"Unexpected message format: {message}\")\n",
    "            continue\n",
    "        if message.get(\"role\") == \"assistant\":\n",
    "            content = message.get(\"content\", \"\")\n",
    "            if not isinstance(content, str):\n",
    "                print(f\"Unexpected content type: {content}\")\n",
    "                continue\n",
    "            num_tokens += len(encoding.encode(content))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    if values:\n",
    "        print(f\"\\n#### Distribution of {name}:\")\n",
    "        print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "        print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "        print(f\"p5 / p95: {np.quantile(values, 0.05)}, {np.quantile(values, 0.95)}\")\n",
    "    else:\n",
    "        print(f\"No values to display for {name}\")\n",
    "\n",
    "files = [\n",
    "    r'C:\\Users\\21629\\Desktop\\new\\train_data.jsonl',\n",
    "    r'C:\\Users\\21629\\Desktop\\new\\val_data.jsonl'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            total_tokens = []\n",
    "            assistant_tokens = []\n",
    "            for line in f:\n",
    "                try:\n",
    "                    ex = json.loads(line)\n",
    "                    messages = ex.get(\"messages\", [])\n",
    "                    if not isinstance(messages, list):\n",
    "                        raise ValueError(\"The 'messages' field should be a list.\")\n",
    "                    total_tokens.append(num_tokens_from_messages(messages))\n",
    "                    assistant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON line: {line}\")\n",
    "                except ValueError as ve:\n",
    "                    print(f\"ValueError: {ve} - line: {line}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error processing line: {e} - line: {line}\")\n",
    "\n",
    "            if total_tokens and assistant_tokens:\n",
    "                print_distribution(total_tokens, \"total tokens\")\n",
    "                print_distribution(assistant_tokens, \"assistant tokens\")\n",
    "            else:\n",
    "                print(\"No valid data to process.\")\n",
    "\n",
    "            print('*' * 50)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-3da8bb89e4944a37a951caf7d5ac82ff\n",
      "Validation file ID: file-7e58ea9ecb6b4016a6962e1a45960997\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_oai_endpoint,\n",
    "    api_key=azure_oai_key,\n",
    "    api_version=\"2024-05-01-preview\"  # Ensure this API version is correct\n",
    ")\n",
    "\n",
    "training_file_name = r'C:\\Users\\21629\\Desktop\\new\\train_data.jsonl'\n",
    "validation_file_name = r'C:\\Users\\21629\\Desktop\\new\\val_data.jsonl'\n",
    "\n",
    "try:\n",
    "    # Upload the training dataset file\n",
    "    with open(training_file_name, \"rb\") as file:\n",
    "        training_response = client.files.create(\n",
    "            file=file, purpose=\"fine-tune\"\n",
    "        )\n",
    "    training_file_id = training_response.id\n",
    "    print(\"Training file ID:\", training_file_id)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading training file: {e}\")\n",
    "\n",
    "try:\n",
    "    # Upload the validation dataset file\n",
    "    with open(validation_file_name, \"rb\") as file:\n",
    "        validation_response = client.files.create(\n",
    "            file=file, purpose=\"fine-tune\"\n",
    "        )\n",
    "    validation_file_id = validation_response.id\n",
    "    print(\"Validation file ID:\", validation_file_id)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading validation file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation file status: processed\n",
      "Training file status: processed\n"
     ]
    }
   ],
   "source": [
    "# List all files\n",
    "#check files status (needs to be Processed)\n",
    "response = client.files.list()\n",
    "\n",
    "# Iterate through the response to check the status of each file\n",
    "for file in response:\n",
    "    if file.id == training_file_id:\n",
    "        print(f\"Training file status: {file.status}\")\n",
    "    elif file.id == validation_file_id:\n",
    "        print(f\"Validation file status: {file.status}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'invalidPayload', 'message': 'The specified model does not exist.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Submit fine-tuning training job\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_file_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_file_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Change to a fine-tuneable model\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m job_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# You can use the job ID to monitor the status of the fine-tuning job.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# The fine-tuning job will take some time to start and complete.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\resources\\fine_tuning\\jobs.py:104\u001b[0m, in \u001b[0;36mJobs.create\u001b[1;34m(self, model, training_file, hyperparameters, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m     51\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FineTuningJob:\n\u001b[0;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    Creates a job that fine-tunes a specified model from a given dataset.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fine_tuning/jobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjob_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJobCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFineTuningJob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[1;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'code': 'invalidPayload', 'message': 'The specified model does not exist.'}}"
     ]
    }
   ],
   "source": [
    "# Submit fine-tuning training job\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file = training_file_id,\n",
    "    validation_file = validation_file_id,\n",
    "    model = \"gpt-3.5-turbo\"  # Change to a fine-tuneable model\n",
    ")\n",
    "\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tuning job.\n",
    "# The fine-tuning job will take some time to start and complete.\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List fine-tuning events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)\n",
    "print(response.model_dump_json(indent=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final training run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve fine_tuned_model name\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n",
    "fine_tuned_model = response.fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy fine-tuned model\n",
    "\n",
    "import json\n",
    "import requests\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"token\")\n",
    "print(token)\n",
    "subscription = os.getenv(\"subscription\")\n",
    "print(subscription)\n",
    "resource_group = os.getenv(\"resource_group\")\n",
    "resource_name = os.getenv(\"resource_name\")\n",
    "model_deployment_name = os.getenv(\"model_deployment_name\")\n",
    "\n",
    "deploy_params = {'api-version': \"2024-05-01-preview\"}\n",
    "deploy_headers = {'Authorization': 'Bearer {}'.format(token), 'Content-Type': 'application/json'}\n",
    "\n",
    "deploy_data = {\n",
    "    \"sku\": {\"name\": \"standard\", \"capacity\": 1},\n",
    "    \"properties\": {\n",
    "        \"model\": {\n",
    "            \"format\": \"OpenAI\",\n",
    "            \"name\": \"gpt-35-turbo-0613.ft-3bf62ca6cb934fa299f25b03b9ead914\", #retrieve this value from the previous call, it will look like gpt-35-turbo-0613.ft-b044a9d3cf9c4228b5d393567f693b83\n",
    "            \"version\": \"1\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "deploy_data = json.dumps(deploy_data)\n",
    "\n",
    "request_url = f'https://management.azure.com/subscriptions/{subscription}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{resource_name}/deployments/{model_deployment_name}'\n",
    "\n",
    "print('Creating a new deployment...')\n",
    "\n",
    "r = requests.put(request_url, params=deploy_params, headers=deploy_headers, data=deploy_data)\n",
    "\n",
    "print(r)\n",
    "print(r.reason)\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"powdered sugar\", \"butter\", \"peanut butter\", \"paraffin\", \"chocolate chips\"]\n"
     ]
    }
   ],
   "source": [
    "# Use the deployed customized model\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"OPENAI_API_BASE_fine_tuning\"),\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY_fine_tuning\"),\n",
    "  api_version = \"2024-05-01-preview\",\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"deployment-finetuning\", # model = \"Custom deployment name you chose for your fine-tuning model\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: couscous\\n\\nIngredients: [\\\"1 box powdered sugar\\\", \\\"8 oz. soft butter\\\", \\\"1 (8 oz.) peanut butter\\\", \\\"paraffin\\\", \\\"12 oz. chocolate chips\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"powdered sugar\\\", \\\"butter\\\", \\\"peanut butter\\\", \\\"paraffin\\\", \\\"chocolate chips\\\"]\"}\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
